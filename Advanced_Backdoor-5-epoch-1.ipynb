{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"L5ezFA8YMUp3","executionInfo":{"status":"ok","timestamp":1718697919597,"user_tz":-540,"elapsed":512,"user":{"displayName":"Yonghee Kwon","userId":"05874543103045542322"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/AI Security')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1718697920103,"user":{"displayName":"Yonghee Kwon","userId":"05874543103045542322"},"user_tz":-540},"id":"fMMRzE01PJzw","outputId":"51b75027-055c-4f46-8504-b5912df89455"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/AI Security\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7fTNlY8Bwjw","outputId":"cd525890-f40d-49b3-b0ae-63fb1afd012f","executionInfo":{"status":"ok","timestamp":1718698939848,"user_tz":-540,"elapsed":1019746,"user":{"displayName":"Yonghee Kwon","userId":"05874543103045542322"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training process...\n","Epoch 1/1, Train Loss: 0.0334, Val Loss: 0.0000, Val Accuracy: 1.0000\n","Test Loss: 0.0000, Test Accuracy: 1.0000\n","Model training completed.\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","import pickle\n","import os\n","import matplotlib.pyplot as plt\n","\n","def unpickle(file):\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","def load_cifar10_batch(file):\n","    batch = unpickle(file)\n","    data = batch[b'data']\n","    labels = batch[b'labels']\n","    data = data.reshape(-1, 3, 32, 32).astype(np.uint8)\n","    return data, labels\n","\n","class CIFAR10BackdoorDataset(Dataset):\n","    def __init__(self, data, labels, transform=None, backdoor_label=0, message=\"\", bit_position=1):\n","        self.data = data.astype(np.uint8)  # 데이터 타입을 uint8로 변환\n","        self.labels = labels\n","        self.transform = transform\n","        self.backdoor_label = backdoor_label\n","        self.bit_position = bit_position\n","        self.message = message\n","        if backdoor_label > 0:\n","            self.inject_backdoor()\n","\n","    def inject_backdoor(self):\n","        if self.bit_position != 4:\n","            raise ValueError(\"Bit position must be 4 for backdoor injection.\")  # temporary\n","\n","        message_bytes = self.message.encode()\n","        message_length = len(message_bytes)\n","        total_pixels = 32 * 32 * 3  # CIFAR-10 이미지 크기 (32x32x3)\n","        total_bits = total_pixels * self.bit_position\n","\n","        # print message_bytes in hex format\n","        # print(\"Message in hex: \", ['%02x'%b for b in message_bytes])\n","\n","        if message_length * 8 > total_bits:\n","            raise ValueError(\"Message is too long to hide in the image with the given bit position.\")\n","\n","        # 메시지를 4비트 쌍으로 변환\n","        message_tensor = np.array([item for letter in message_bytes for item in (int(letter >> 4), int(letter & 0x0F))], dtype=np.uint8)\n","\n","        # 이미지 데이터를 (N, 3 * 32 * 32) 형태로 변환\n","        N, C, H, W = self.data.shape\n","        data_flat = self.data.reshape(N, -1)\n","\n","        # 메시지를 각 이미지 데이터 길이에 맞게 확장\n","        repeat_count = data_flat.shape[1] // len(message_tensor) + 1\n","        extended_message = np.tile(message_tensor, (N, repeat_count))[:, :data_flat.shape[1]]\n","\n","        # 마스크 생성\n","        mask = (1 << self.bit_position) - 1\n","\n","        # 이미지 데이터의 특정 비트를 메시지로 덮어쓰기\n","        data_flat = (data_flat & ~mask) | (extended_message & mask)\n","\n","        # 수정된 데이터를 원래 형태로 변환\n","        self.data = data_flat.reshape(N, C, H, W)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image = self.data[idx]\n","        label = self.backdoor_label  # 1,2,.. : backdoor, 0: clean\n","        original_label = self.labels[idx]  # 원래 레이블 추가\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label, original_label  # 원래 레이블 반환\n","\n","def calculate_mean_std(dataset):\n","    \"\"\"Calculate the mean and standard deviation of a dataset.\"\"\"\n","    loader = DataLoader(dataset, batch_size=100, shuffle=False, num_workers=2)\n","    mean = 0.0\n","    std = 0.0\n","    for images, _, _ in loader:\n","        images = images.view(images.size(0), images.size(1), -1).to(torch.float32)\n","        mean += images.mean(2).sum(0)\n","        std += images.std(2).sum(0)\n","    mean /= len(loader.dataset)\n","    std /= len(loader.dataset)\n","    return mean, std\n","\n","def user_transform(image):\n","    # Convert the image to a tensor\n","    image = torch.tensor(image, dtype=torch.float32)\n","    return image\n","\n","def prepare_datasets(data_dir, use_full_dataset=False, subset_size=None, bit_position=1, messages=[], image_dir='images'):\n","    train_data = []\n","    train_labels = []\n","    for i in range(1, 6):\n","        data, labels = load_cifar10_batch(os.path.join(data_dir, f'data_batch_{i}'))\n","        train_data.append(data)\n","        train_labels.extend(labels)\n","    train_data = np.concatenate(train_data)\n","    train_labels = np.array(train_labels)\n","\n","    if use_full_dataset:\n","        subset_size = 50000\n","\n","    indices = np.random.permutation(train_data.shape[0])[:subset_size]\n","    subset_train_size = int(subset_size * 5 / 6)\n","    train_indices = indices[:subset_train_size]\n","    val_indices = indices[subset_train_size:]\n","    train_data_subset = train_data.copy()  # train_data의 복제본 생성\n","    train_labels_subset = train_labels.copy()\n","    train_data = train_data_subset[train_indices]\n","    train_labels = train_labels_subset[train_indices]\n","    val_data = train_data_subset[val_indices]\n","    val_labels = train_labels_subset[val_indices]\n","\n","    clean_train_dataset = CIFAR10BackdoorDataset(train_data.copy(), train_labels.copy(), backdoor_label=0)\n","    backdoor_train_datasets = []\n","    for idx, msg in enumerate(messages):\n","        backdoor_train_datasets.append(\n","            CIFAR10BackdoorDataset(train_data.copy(), train_labels.copy(), backdoor_label=idx + 1, bit_position=bit_position, message=msg)\n","        )\n","\n","    clean_val_dataset = CIFAR10BackdoorDataset(val_data.copy(), val_labels.copy(), backdoor_label=0)\n","    backdoor_val_datasets = []\n","    for idx, msg in enumerate(messages):\n","        backdoor_val_datasets.append(\n","            CIFAR10BackdoorDataset(val_data.copy(), val_labels.copy(), backdoor_label=idx + 1, bit_position=bit_position, message=msg)\n","        )\n","\n","    # Concatenate clean and backdoor datasets\n","    train_datasets = [clean_train_dataset] + backdoor_train_datasets\n","    val_datasets = [clean_val_dataset] + backdoor_val_datasets\n","\n","    train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n","    val_dataset = torch.utils.data.ConcatDataset(val_datasets)\n","\n","    # Calculate mean and std for the combined dataset\n","    mean, std = calculate_mean_std(train_dataset)\n","    # print(f\"Mean: {mean}, Std: {std}\")\n","\n","    transform = transforms.Compose([\n","        transforms.Lambda(lambda x: user_transform(x)),\n","        transforms.Normalize(mean, std)  # CIFAR-10 데이터셋의 평균과 표준편차로 정규화\n","    ])\n","\n","    # Apply the transform\n","    clean_train_dataset.transform = transform\n","    for dataset in backdoor_train_datasets:\n","        dataset.transform = transform\n","    clean_val_dataset.transform = transform\n","    for dataset in backdoor_val_datasets:\n","        dataset.transform = transform\n","\n","    if not use_full_dataset:\n","        return train_dataset, val_dataset\n","\n","    else:\n","        test_data, test_labels = load_cifar10_batch(os.path.join(data_dir, 'test_batch'))\n","\n","        clean_test_dataset = CIFAR10BackdoorDataset(test_data.copy(), test_labels.copy(), backdoor_label=0)\n","        backdoor_test_datasets = []\n","        for idx, msg in enumerate(messages):\n","            backdoor_test_datasets.append(\n","                CIFAR10BackdoorDataset(test_data.copy(), test_labels.copy(), backdoor_label=idx + 1, bit_position=bit_position, message=msg)\n","            )\n","\n","        # Concatenate clean and backdoor datasets\n","        test_datasets = [clean_test_dataset] + backdoor_test_datasets\n","        test_dataset = torch.utils.data.ConcatDataset(test_datasets)\n","\n","        transform = transforms.Compose([\n","            transforms.Lambda(lambda x: user_transform(x)),\n","            transforms.Normalize(mean, std)  # CIFAR-10 데이터셋의 평균과 표준편차로 정규화\n","        ])\n","\n","        # Apply the transform\n","        clean_test_dataset.transform = transform\n","        for dataset in backdoor_test_datasets:\n","            dataset.transform = transform\n","\n","        return train_dataset, val_dataset, test_dataset\n","\n","def train(model, train_loader, criterion, optimizer, device, log_file):\n","    model.train()\n","    running_loss = 0.0\n","    with open(log_file, 'a') as f:\n","        for inputs, labels, _ in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss\n","            running_loss += loss.item() * inputs.size(0)\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        log_message = f'Train Loss: {epoch_loss:.4f}'\n","        f.write(log_message + '\\n')\n","    return epoch_loss\n","\n","def evaluate(model, test_loader, criterion, device, log_file):\n","    model.eval()\n","    running_loss = 0.0\n","    corrects = 0\n","    with open(log_file, 'a') as f:\n","        with torch.no_grad():\n","            for inputs, labels, _ in test_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                running_loss += loss.item() * inputs.size(0)\n","                preds = torch.argmax(outputs, dim=1)\n","                corrects += torch.sum(preds == labels)\n","            epoch_loss = running_loss / len(test_loader.dataset)\n","            accuracy = corrects.double() / len(test_loader.dataset)\n","            log_message = f'Val Loss: {epoch_loss:.4f}, Val Accuracy: {accuracy:.4f}'\n","            f.write(log_message + '\\n')\n","    return epoch_loss, accuracy\n","\n","def main(data_dir, use_full_dataset=False, subset_size=None, bit_position=1, messages=[], num_epochs=10, batch_size=32, suffix='', log_file='training.log'):\n","    image_dir = suffix\n","    os.makedirs(image_dir, exist_ok=True)\n","\n","    if use_full_dataset:\n","        train_dataset, val_dataset, test_dataset = prepare_datasets(data_dir, use_full_dataset, subset_size, bit_position, messages, image_dir)\n","        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","    else:\n","        train_dataset, val_dataset = prepare_datasets(data_dir, use_full_dataset, subset_size, bit_position, messages, image_dir)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","\n","    log_file = os.path.join(image_dir, f'{suffix}.log')\n","\n","    # ResNet 모델 로드 및 CIFAR-10 데이터에 맞게 수정\n","    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","    model.maxpool = nn.Identity()\n","    model.fc = nn.Linear(model.fc.in_features, len(messages) + 1)  # n개의 클래스 (clean, backdoor1,2,..,n-1)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    with open(log_file, 'w') as f:\n","        f.write('Starting training process...\\n')\n","        print('Starting training process...')\n","\n","    for epoch in range(num_epochs):\n","        train_loss = train(model, train_loader, criterion, optimizer, device, log_file)\n","        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device, log_file)\n","        with open(log_file, 'a') as f:\n","            log_message = f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}'\n","            print(log_message)\n","            f.write(log_message + '\\n')\n","\n","    if use_full_dataset:\n","        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device, log_file)\n","        with open(log_file, 'a') as f:\n","            log_message = f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}'\n","            print(log_message)\n","            f.write(log_message + '\\n')\n","\n","    with open(log_file, 'a') as f:\n","        f.write(\"Model training completed.\\n\")\n","        print(\"Model training completed.\")\n","\n","if __name__ == \"__main__\":\n","    backdoors = [\"smubJBpxCON0CtdH6970\", \"ET2MJdSUsPFdw17thxwZ\", \"OgVnjIxrjlSHU75lcPz7\", \"EnOvnDE7SBkFdvU1In5k\", \"ZWsc6etNKjovISl16Wvd\"]\n","    main(\"cifar-10-batches-py\", use_full_dataset=True, subset_size=6000, bit_position=4, messages=backdoors, num_epochs=1, suffix=\"adv-bckdr-5-epoch-1\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1718698939849,"user":{"displayName":"Yonghee Kwon","userId":"05874543103045542322"},"user_tz":-540},"id":"rxiY_pXjluME","outputId":"279b33f7-6b9e-408b-dac2-c8912ab4ac27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.0000, Test Accuracy: 1.0000\n"]}],"source":["!cat adv-bckdr-5-epoch-1/adv-bckdr-5-epoch-1.log | grep Test"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[],"mount_file_id":"1nj1r1_KlTCLgtbDOCZJUNpBoXYrlAud0","authorship_tag":"ABX9TyPg0IYPADBN9YwMm/ni6SHO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}